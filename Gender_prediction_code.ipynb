{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\majorprojj\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.add(Conv2D(32,3,3,input_shape=(64,64,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization(axis=-1))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=128,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 21, 21, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 10, 10, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 2, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1, 1, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 1, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 24,129\n",
      "Trainable params: 23,937\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda\\envs\\majorprojj\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20856 images belonging to 2 classes.\n",
      "Found 3251 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "x_train = train_datagen.flow_from_directory(r'D:/Hackathon/Training set',target_size=(64,64),batch_size=32,class_mode='binary')\n",
    "x_test = train_datagen.flow_from_directory(r'D:/Hackathon/Test set',target_size=(64,64),batch_size=32,class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'female': 0, 'male': 1}\n"
     ]
    }
   ],
   "source": [
    "print(x_train.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "651/651 [==============================] - 278s 427ms/step - loss: 0.6536 - acc: 0.6203 - val_loss: 0.5328 - val_acc: 0.7466\n",
      "Epoch 2/120\n",
      "651/651 [==============================] - 212s 325ms/step - loss: 0.6002 - acc: 0.6723 - val_loss: 0.4969 - val_acc: 0.7748\n",
      "Epoch 3/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.5815 - acc: 0.6866 - val_loss: 0.5902 - val_acc: 0.6730\n",
      "Epoch 4/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.5596 - acc: 0.7061 - val_loss: 0.6262 - val_acc: 0.6312\n",
      "Epoch 5/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.5568 - acc: 0.7109 - val_loss: 0.6080 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.5467 - acc: 0.7182 - val_loss: 0.4436 - val_acc: 0.7877\n",
      "Epoch 7/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.5429 - acc: 0.7209 - val_loss: 0.4323 - val_acc: 0.8085\n",
      "Epoch 8/120\n",
      "651/651 [==============================] - 209s 322ms/step - loss: 0.5310 - acc: 0.7318 - val_loss: 0.4327 - val_acc: 0.7992\n",
      "Epoch 9/120\n",
      "651/651 [==============================] - 6587s 10s/step - loss: 0.5278 - acc: 0.7303 - val_loss: 0.4590 - val_acc: 0.7735\n",
      "Epoch 10/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.5218 - acc: 0.7358 - val_loss: 0.4928 - val_acc: 0.7642\n",
      "Epoch 11/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.5182 - acc: 0.7358 - val_loss: 0.4987 - val_acc: 0.7429\n",
      "Epoch 12/120\n",
      "651/651 [==============================] - 208s 319ms/step - loss: 0.5154 - acc: 0.7369 - val_loss: 0.4078 - val_acc: 0.8171\n",
      "Epoch 13/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.5117 - acc: 0.7411 - val_loss: 0.4293 - val_acc: 0.7973\n",
      "Epoch 14/120\n",
      "651/651 [==============================] - 215s 330ms/step - loss: 0.5103 - acc: 0.7431 - val_loss: 0.4033 - val_acc: 0.8270\n",
      "Epoch 15/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.5076 - acc: 0.7459 - val_loss: 0.4301 - val_acc: 0.8066\n",
      "Epoch 16/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.5068 - acc: 0.7461 - val_loss: 0.4054 - val_acc: 0.8125\n",
      "Epoch 17/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.5035 - acc: 0.7488 - val_loss: 0.3793 - val_acc: 0.8280\n",
      "Epoch 18/120\n",
      "651/651 [==============================] - 209s 320ms/step - loss: 0.5014 - acc: 0.7530 - val_loss: 0.4519 - val_acc: 0.7822\n",
      "Epoch 19/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4974 - acc: 0.7543 - val_loss: 0.3984 - val_acc: 0.8351\n",
      "Epoch 20/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4932 - acc: 0.7560 - val_loss: 0.4429 - val_acc: 0.7899\n",
      "Epoch 21/120\n",
      "651/651 [==============================] - 209s 322ms/step - loss: 0.4957 - acc: 0.7545 - val_loss: 0.3686 - val_acc: 0.8391\n",
      "Epoch 22/120\n",
      "651/651 [==============================] - 208s 319ms/step - loss: 0.4916 - acc: 0.7563 - val_loss: 0.4366 - val_acc: 0.8017\n",
      "Epoch 23/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4967 - acc: 0.7535 - val_loss: 0.4216 - val_acc: 0.8140\n",
      "Epoch 24/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4908 - acc: 0.7573 - val_loss: 0.4403 - val_acc: 0.7896\n",
      "Epoch 25/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4892 - acc: 0.7576 - val_loss: 0.3943 - val_acc: 0.8199\n",
      "Epoch 26/120\n",
      "651/651 [==============================] - 209s 322ms/step - loss: 0.4848 - acc: 0.7627 - val_loss: 0.4460 - val_acc: 0.7744\n",
      "Epoch 27/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4876 - acc: 0.7570 - val_loss: 0.4222 - val_acc: 0.8011\n",
      "Epoch 28/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4857 - acc: 0.7595 - val_loss: 0.4108 - val_acc: 0.8156\n",
      "Epoch 29/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4851 - acc: 0.7595 - val_loss: 0.4140 - val_acc: 0.8029\n",
      "Epoch 30/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4824 - acc: 0.7587 - val_loss: 0.3922 - val_acc: 0.8493\n",
      "Epoch 31/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4853 - acc: 0.7588 - val_loss: 0.3927 - val_acc: 0.8351\n",
      "Epoch 32/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4755 - acc: 0.7673 - val_loss: 0.3854 - val_acc: 0.8342\n",
      "Epoch 33/120\n",
      "651/651 [==============================] - 229s 351ms/step - loss: 0.4783 - acc: 0.7611 - val_loss: 0.3662 - val_acc: 0.8373\n",
      "Epoch 34/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.4846 - acc: 0.7572 - val_loss: 0.4086 - val_acc: 0.8270\n",
      "Epoch 35/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4732 - acc: 0.7670 - val_loss: 0.3416 - val_acc: 0.8496\n",
      "Epoch 36/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4761 - acc: 0.7657 - val_loss: 0.3643 - val_acc: 0.8416\n",
      "Epoch 37/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.4759 - acc: 0.7659 - val_loss: 0.4535 - val_acc: 0.7803\n",
      "Epoch 38/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4822 - acc: 0.7597 - val_loss: 0.4195 - val_acc: 0.8110\n",
      "Epoch 39/120\n",
      "651/651 [==============================] - 209s 322ms/step - loss: 0.4754 - acc: 0.7663 - val_loss: 0.3722 - val_acc: 0.8351\n",
      "Epoch 40/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4802 - acc: 0.7655 - val_loss: 0.3986 - val_acc: 0.8286\n",
      "Epoch 41/120\n",
      "651/651 [==============================] - 209s 320ms/step - loss: 0.4725 - acc: 0.7676 - val_loss: 0.3598 - val_acc: 0.8465\n",
      "Epoch 42/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4702 - acc: 0.7660 - val_loss: 0.3906 - val_acc: 0.8320\n",
      "Epoch 43/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4710 - acc: 0.7666 - val_loss: 0.3792 - val_acc: 0.8419\n",
      "Epoch 44/120\n",
      "651/651 [==============================] - 211s 325ms/step - loss: 0.4691 - acc: 0.7700 - val_loss: 0.3656 - val_acc: 0.8422\n",
      "Epoch 45/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4781 - acc: 0.7670 - val_loss: 0.3526 - val_acc: 0.8493\n",
      "Epoch 46/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4687 - acc: 0.7704 - val_loss: 0.3938 - val_acc: 0.8236\n",
      "Epoch 47/120\n",
      "651/651 [==============================] - 211s 325ms/step - loss: 0.4681 - acc: 0.7694 - val_loss: 0.3953 - val_acc: 0.8202\n",
      "Epoch 48/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4679 - acc: 0.7698 - val_loss: 0.3628 - val_acc: 0.8552\n",
      "Epoch 49/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4673 - acc: 0.7717 - val_loss: 0.4793 - val_acc: 0.7584\n",
      "Epoch 50/120\n",
      "651/651 [==============================] - 207s 319ms/step - loss: 0.4720 - acc: 0.7661 - val_loss: 0.3536 - val_acc: 0.8475\n",
      "Epoch 51/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4674 - acc: 0.7750 - val_loss: 0.3736 - val_acc: 0.8382\n",
      "Epoch 52/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4678 - acc: 0.7677 - val_loss: 0.3395 - val_acc: 0.8589\n",
      "Epoch 53/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4680 - acc: 0.7699 - val_loss: 0.3907 - val_acc: 0.8181\n",
      "Epoch 54/120\n",
      "651/651 [==============================] - 208s 319ms/step - loss: 0.4623 - acc: 0.7759 - val_loss: 0.3837 - val_acc: 0.8373\n",
      "Epoch 55/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4643 - acc: 0.7732 - val_loss: 0.3528 - val_acc: 0.8564\n",
      "Epoch 56/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4654 - acc: 0.7714 - val_loss: 0.3804 - val_acc: 0.8373\n",
      "Epoch 57/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4680 - acc: 0.7704 - val_loss: 0.3979 - val_acc: 0.8224\n",
      "Epoch 58/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4663 - acc: 0.7715 - val_loss: 0.3496 - val_acc: 0.8481\n",
      "Epoch 59/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4633 - acc: 0.7752 - val_loss: 0.3465 - val_acc: 0.8574\n",
      "Epoch 60/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4695 - acc: 0.7699 - val_loss: 0.3711 - val_acc: 0.8335\n",
      "Epoch 61/120\n",
      "651/651 [==============================] - 209s 320ms/step - loss: 0.4649 - acc: 0.7723 - val_loss: 0.3562 - val_acc: 0.8527\n",
      "Epoch 62/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4677 - acc: 0.7696 - val_loss: 0.3654 - val_acc: 0.8434\n",
      "Epoch 63/120\n",
      "651/651 [==============================] - 212s 326ms/step - loss: 0.4611 - acc: 0.7759 - val_loss: 0.3790 - val_acc: 0.8249\n",
      "Epoch 64/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4615 - acc: 0.7738 - val_loss: 0.3670 - val_acc: 0.8354\n",
      "Epoch 65/120\n",
      "651/651 [==============================] - 209s 320ms/step - loss: 0.4622 - acc: 0.7726 - val_loss: 0.4006 - val_acc: 0.8140\n",
      "Epoch 66/120\n",
      "651/651 [==============================] - 208s 319ms/step - loss: 0.4588 - acc: 0.7754 - val_loss: 0.3497 - val_acc: 0.8598\n",
      "Epoch 67/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4603 - acc: 0.7762 - val_loss: 0.3805 - val_acc: 0.8329\n",
      "Epoch 68/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4644 - acc: 0.7726 - val_loss: 0.3500 - val_acc: 0.8493\n",
      "Epoch 69/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4590 - acc: 0.7776 - val_loss: 0.3487 - val_acc: 0.8472\n",
      "Epoch 70/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4588 - acc: 0.7753 - val_loss: 0.4192 - val_acc: 0.8301\n",
      "Epoch 71/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4603 - acc: 0.7760 - val_loss: 0.3551 - val_acc: 0.8456\n",
      "Epoch 72/120\n",
      "651/651 [==============================] - 205s 315ms/step - loss: 0.4563 - acc: 0.7779 - val_loss: 0.3353 - val_acc: 0.8632\n",
      "Epoch 73/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4620 - acc: 0.7737 - val_loss: 0.4116 - val_acc: 0.8156\n",
      "Epoch 74/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4596 - acc: 0.7761 - val_loss: 0.4025 - val_acc: 0.8258\n",
      "Epoch 75/120\n",
      "651/651 [==============================] - 205s 315ms/step - loss: 0.4612 - acc: 0.7744 - val_loss: 0.3377 - val_acc: 0.8558\n",
      "Epoch 76/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4571 - acc: 0.7780 - val_loss: 0.3555 - val_acc: 0.8478\n",
      "Epoch 77/120\n",
      "651/651 [==============================] - 229s 351ms/step - loss: 0.4659 - acc: 0.7746 - val_loss: 0.4032 - val_acc: 0.8134\n",
      "Epoch 78/120\n",
      "651/651 [==============================] - 211s 325ms/step - loss: 0.4587 - acc: 0.7772 - val_loss: 0.3314 - val_acc: 0.8608\n",
      "Epoch 79/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4605 - acc: 0.7749 - val_loss: 0.3592 - val_acc: 0.8391\n",
      "Epoch 80/120\n",
      "651/651 [==============================] - 212s 325ms/step - loss: 0.4597 - acc: 0.7753 - val_loss: 0.3903 - val_acc: 0.8178\n",
      "Epoch 81/120\n",
      "651/651 [==============================] - 241s 370ms/step - loss: 0.4512 - acc: 0.7776 - val_loss: 0.3558 - val_acc: 0.8413\n",
      "Epoch 82/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4621 - acc: 0.7752 - val_loss: 0.3516 - val_acc: 0.8487\n",
      "Epoch 83/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4531 - acc: 0.7781 - val_loss: 0.3624 - val_acc: 0.8388\n",
      "Epoch 84/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4534 - acc: 0.7802 - val_loss: 0.3393 - val_acc: 0.8527\n",
      "Epoch 85/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.4525 - acc: 0.7810 - val_loss: 0.3535 - val_acc: 0.8459\n",
      "Epoch 86/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4530 - acc: 0.7781 - val_loss: 0.4292 - val_acc: 0.7933\n",
      "Epoch 87/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4550 - acc: 0.7833 - val_loss: 0.3527 - val_acc: 0.8549\n",
      "Epoch 88/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4507 - acc: 0.7839 - val_loss: 0.3832 - val_acc: 0.8280\n",
      "Epoch 89/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4541 - acc: 0.7777 - val_loss: 0.4043 - val_acc: 0.8159\n",
      "Epoch 90/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4562 - acc: 0.7784 - val_loss: 0.3546 - val_acc: 0.8530\n",
      "Epoch 91/120\n",
      "651/651 [==============================] - 209s 322ms/step - loss: 0.4549 - acc: 0.7764 - val_loss: 0.3762 - val_acc: 0.8357\n",
      "Epoch 92/120\n",
      "651/651 [==============================] - 211s 325ms/step - loss: 0.4560 - acc: 0.7753 - val_loss: 0.3954 - val_acc: 0.8280\n",
      "Epoch 93/120\n",
      "651/651 [==============================] - 207s 319ms/step - loss: 0.4559 - acc: 0.7762 - val_loss: 0.3920 - val_acc: 0.8227\n",
      "Epoch 94/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4521 - acc: 0.7818 - val_loss: 0.3395 - val_acc: 0.8632\n",
      "Epoch 95/120\n",
      "651/651 [==============================] - 213s 327ms/step - loss: 0.4534 - acc: 0.7809 - val_loss: 0.3516 - val_acc: 0.8549\n",
      "Epoch 96/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4552 - acc: 0.7792 - val_loss: 0.3656 - val_acc: 0.8478\n",
      "Epoch 97/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4528 - acc: 0.7803 - val_loss: 0.3398 - val_acc: 0.8521\n",
      "Epoch 98/120\n",
      "651/651 [==============================] - 212s 325ms/step - loss: 0.4588 - acc: 0.7778 - val_loss: 0.3578 - val_acc: 0.8459\n",
      "Epoch 99/120\n",
      "651/651 [==============================] - 209s 322ms/step - loss: 0.4519 - acc: 0.7801 - val_loss: 0.3321 - val_acc: 0.8567\n",
      "Epoch 100/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4550 - acc: 0.7816 - val_loss: 0.4096 - val_acc: 0.8088\n",
      "Epoch 101/120\n",
      "651/651 [==============================] - 209s 321ms/step - loss: 0.4483 - acc: 0.7822 - val_loss: 0.4026 - val_acc: 0.7970\n",
      "Epoch 102/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.4513 - acc: 0.7819 - val_loss: 0.3412 - val_acc: 0.8543\n",
      "Epoch 103/120\n",
      "651/651 [==============================] - 207s 318ms/step - loss: 0.4495 - acc: 0.7831 - val_loss: 0.3682 - val_acc: 0.8385\n",
      "Epoch 104/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4503 - acc: 0.7811 - val_loss: 0.3413 - val_acc: 0.8586\n",
      "Epoch 105/120\n",
      "651/651 [==============================] - 212s 325ms/step - loss: 0.4544 - acc: 0.7801 - val_loss: 0.3409 - val_acc: 0.8521\n",
      "Epoch 106/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.4538 - acc: 0.7810 - val_loss: 0.3492 - val_acc: 0.8583\n",
      "Epoch 107/120\n",
      "651/651 [==============================] - 208s 319ms/step - loss: 0.4519 - acc: 0.7818 - val_loss: 0.3913 - val_acc: 0.8140\n",
      "Epoch 108/120\n",
      "651/651 [==============================] - 210s 322ms/step - loss: 0.4498 - acc: 0.7807 - val_loss: 0.3842 - val_acc: 0.8224\n",
      "Epoch 109/120\n",
      "651/651 [==============================] - 209s 322ms/step - loss: 0.4502 - acc: 0.7809 - val_loss: 0.3623 - val_acc: 0.8385\n",
      "Epoch 110/120\n",
      "651/651 [==============================] - 211s 323ms/step - loss: 0.4464 - acc: 0.7840 - val_loss: 0.3583 - val_acc: 0.8366\n",
      "Epoch 111/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4535 - acc: 0.7805 - val_loss: 0.3697 - val_acc: 0.8425\n",
      "Epoch 112/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4535 - acc: 0.7809 - val_loss: 0.3782 - val_acc: 0.8264\n",
      "Epoch 113/120\n",
      "651/651 [==============================] - 211s 324ms/step - loss: 0.4436 - acc: 0.7844 - val_loss: 0.3365 - val_acc: 0.8533\n",
      "Epoch 114/120\n",
      "651/651 [==============================] - 214s 328ms/step - loss: 0.4528 - acc: 0.7810 - val_loss: 0.3528 - val_acc: 0.8468\n",
      "Epoch 115/120\n",
      "651/651 [==============================] - 214s 329ms/step - loss: 0.4508 - acc: 0.7802 - val_loss: 0.3428 - val_acc: 0.8475\n",
      "Epoch 116/120\n",
      "651/651 [==============================] - 208s 320ms/step - loss: 0.4495 - acc: 0.7866 - val_loss: 0.3404 - val_acc: 0.8484\n",
      "Epoch 117/120\n",
      "651/651 [==============================] - 210s 323ms/step - loss: 0.4486 - acc: 0.7827 - val_loss: 0.3551 - val_acc: 0.8444\n",
      "Epoch 118/120\n",
      "651/651 [==============================] - 211s 325ms/step - loss: 0.4440 - acc: 0.7869 - val_loss: 0.3558 - val_acc: 0.8496\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651/651 [==============================] - 278s 427ms/step - loss: 0.4511 - acc: 0.7816 - val_loss: 0.3484 - val_acc: 0.8521\n",
      "Epoch 120/120\n",
      "651/651 [==============================] - 459s 705ms/step - loss: 0.4517 - acc: 0.7803 - val_loss: 0.3470 - val_acc: 0.8549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28867d59548>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(x_train,steps_per_epoch=x_train.samples//32,epochs=120,validation_data=x_test,validation_steps=x_test.samples//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gender.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
